{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e2d85d",
   "metadata": {},
   "source": [
    "# 깊은 다층 퍼셉트론 (Deep Multilayer Perceptron)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 다층 퍼셉트론(MLP)이란?\n",
    "\n",
    "- **퍼셉트론(Perceptron)** : 인공 뉴런 1개에 해당하는 가장 기본적인 단위.\n",
    "- **다층 퍼셉트론(MLP)** : 퍼셉트론을 여러 층으로 쌓아 만든 인공 신경망.\n",
    "  - 입력층(Input layer) → 은닉층(Hidden layers) → 출력층(Output layer)으로 구성.\n",
    "  - 은닉층이 하나라도 있으면 ‘다층 퍼셉트론’이라고 부름.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 왜 ‘깊은’ MLP인가?\n",
    "\n",
    "- **깊다(Deep)** 는 은닉층(Hidden layer)이 여러 개인 경우를 의미.\n",
    "- 은닉층이 많을수록 복잡하고 추상적인 패턴까지 학습 가능.\n",
    "- 단층 퍼셉트론으로는 선형 문제만 풀 수 있지만,\n",
    "  다층으로 쌓으면 비선형 문제도 해결할 수 있음.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 구조 간단히 보기\n",
    "\n",
    "- **입력층** : 원시 데이터(숫자, 이미지 등)를 받아옴.\n",
    "- **은닉층** : 뉴런과 활성화 함수로 구성. 정보 변환 및 패턴 학습.\n",
    "- **출력층** : 문제에 맞는 결과 출력 (예: 클래스 확률).\n",
    "\n",
    "## 4. 주요 원리\n",
    "\n",
    "- **Forward Propagation (순전파)** :\n",
    "  - 입력 → 가중치(W) 곱하기 → 편향(b) 더하기 → 활성화 함수 → 출력.\n",
    "  - 층을 따라 결과가 전달됨.\n",
    "\n",
    "- **Activation Function (활성화 함수)** :\n",
    "  - 뉴런의 출력에 비선형성을 추가해 복잡한 함수 근사 가능.\n",
    "  - 예: ReLU, Sigmoid, Tanh 등.\n",
    "\n",
    "- **Loss Function (손실 함수)** :\n",
    "  - 출력값과 실제 정답의 차이 계산.\n",
    "  - 손실을 줄이도록 모델을 학습시킴.\n",
    "\n",
    "- **Backpropagation (역전파)** :\n",
    "  - 손실 함수의 미분값을 이용해 가중치와 편향을 조정.\n",
    "  - 경사하강법(Gradient Descent) 기반.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. MLP의 특징\n",
    "\n",
    "| 특징 | 설명 |\n",
    "|------|------|\n",
    "| Fully Connected | 모든 뉴런이 이전 층의 모든 뉴런과 연결됨 |\n",
    "| 비선형성 | 활성화 함수로 비선형 패턴 학습 가능 |\n",
    "| 입력 크기 고정 | 입력은 벡터 형태여야 함 (이미지는 flatten해서 사용) |\n",
    "| 활용 예시 | 숫자 분류(MNIST), 간단한 회귀문제 등 |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 실제 예: MNIST 숫자 분류\n",
    "\n",
    "- 입력: 28x28 흑백 이미지 → 784차원 벡터.\n",
    "- 은닉층 예시: [512 → 256 → 128] 뉴런.\n",
    "- 출력층: 0~9 숫자 → 10개 노드 + softmax.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 장점과 한계\n",
    "\n",
    "**장점**\n",
    "- 구조가 직관적이고 구현이 간단함.\n",
    "- tabular 데이터, 기본적인 패턴 학습에 적합.\n",
    "\n",
    "**한계**\n",
    "- 입력 데이터가 벡터형이라 이미지/시계열에는 CNN, RNN이 더 적합.\n",
    "- 층이 깊어지면 Gradient Vanishing 문제가 발생할 수 있음.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. 한 문장 요약\n",
    "\n",
    "> **깊은 다층 퍼셉트론(Deep MLP)** 은 뉴런과 은닉층을 여러 겹 쌓아 복잡한 패턴과 비선형성을 학습할 수 있는 가장 기본적인 인공 신경망\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea99c19",
   "metadata": {},
   "source": [
    "| 구성 요소          | 역할                  | 주요 특징                     |\n",
    "| -------------- | ------------------- | ------------------------- |\n",
    "| **Sequential** | 레이어를 순차적으로 쌓는 기본 모델 | 직관적, 간단                   |\n",
    "| **Dense**      | 완전 연결층              | 입력과 출력 노드 모두 연결, 활성 함수 사용 |\n",
    "| **SGD**        | 가중치 업데이트 방법         | 학습률로 경사하강, 딥러닝의 기본 옵티마이저  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6937152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 - 1s - loss: 0.0899 - accuracy: 0.1554 - val_loss: 0.0868 - val_accuracy: 0.2669 - 1s/epoch - 3ms/step\n",
      "Epoch 2/50\n",
      "469/469 - 1s - loss: 0.0833 - accuracy: 0.3833 - val_loss: 0.0793 - val_accuracy: 0.4735 - 895ms/epoch - 2ms/step\n",
      "Epoch 3/50\n",
      "469/469 - 1s - loss: 0.0754 - accuracy: 0.5064 - val_loss: 0.0712 - val_accuracy: 0.5369 - 897ms/epoch - 2ms/step\n",
      "Epoch 4/50\n",
      "469/469 - 1s - loss: 0.0679 - accuracy: 0.5563 - val_loss: 0.0642 - val_accuracy: 0.5782 - 925ms/epoch - 2ms/step\n",
      "Epoch 5/50\n",
      "469/469 - 1s - loss: 0.0618 - accuracy: 0.5917 - val_loss: 0.0587 - val_accuracy: 0.6091 - 917ms/epoch - 2ms/step\n",
      "Epoch 6/50\n",
      "469/469 - 1s - loss: 0.0569 - accuracy: 0.6208 - val_loss: 0.0542 - val_accuracy: 0.6359 - 930ms/epoch - 2ms/step\n",
      "Epoch 7/50\n",
      "469/469 - 1s - loss: 0.0527 - accuracy: 0.6546 - val_loss: 0.0503 - val_accuracy: 0.6777 - 916ms/epoch - 2ms/step\n",
      "Epoch 8/50\n",
      "469/469 - 1s - loss: 0.0491 - accuracy: 0.6917 - val_loss: 0.0467 - val_accuracy: 0.7199 - 930ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "469/469 - 1s - loss: 0.0458 - accuracy: 0.7258 - val_loss: 0.0436 - val_accuracy: 0.7534 - 928ms/epoch - 2ms/step\n",
      "Epoch 10/50\n",
      "469/469 - 1s - loss: 0.0430 - accuracy: 0.7543 - val_loss: 0.0408 - val_accuracy: 0.7795 - 936ms/epoch - 2ms/step\n",
      "Epoch 11/50\n",
      "469/469 - 1s - loss: 0.0404 - accuracy: 0.7799 - val_loss: 0.0383 - val_accuracy: 0.8053 - 926ms/epoch - 2ms/step\n",
      "Epoch 12/50\n",
      "469/469 - 1s - loss: 0.0381 - accuracy: 0.8004 - val_loss: 0.0361 - val_accuracy: 0.8213 - 934ms/epoch - 2ms/step\n",
      "Epoch 13/50\n",
      "469/469 - 1s - loss: 0.0361 - accuracy: 0.8135 - val_loss: 0.0342 - val_accuracy: 0.8303 - 928ms/epoch - 2ms/step\n",
      "Epoch 14/50\n",
      "469/469 - 1s - loss: 0.0343 - accuracy: 0.8232 - val_loss: 0.0325 - val_accuracy: 0.8381 - 942ms/epoch - 2ms/step\n",
      "Epoch 15/50\n",
      "469/469 - 1s - loss: 0.0328 - accuracy: 0.8309 - val_loss: 0.0311 - val_accuracy: 0.8446 - 972ms/epoch - 2ms/step\n",
      "Epoch 16/50\n",
      "469/469 - 1s - loss: 0.0315 - accuracy: 0.8368 - val_loss: 0.0299 - val_accuracy: 0.8503 - 1s/epoch - 2ms/step\n",
      "Epoch 17/50\n",
      "469/469 - 1s - loss: 0.0304 - accuracy: 0.8418 - val_loss: 0.0288 - val_accuracy: 0.8539 - 946ms/epoch - 2ms/step\n",
      "Epoch 18/50\n",
      "469/469 - 1s - loss: 0.0294 - accuracy: 0.8457 - val_loss: 0.0278 - val_accuracy: 0.8576 - 952ms/epoch - 2ms/step\n",
      "Epoch 19/50\n",
      "469/469 - 1s - loss: 0.0285 - accuracy: 0.8492 - val_loss: 0.0270 - val_accuracy: 0.8606 - 939ms/epoch - 2ms/step\n",
      "Epoch 20/50\n",
      "469/469 - 1s - loss: 0.0277 - accuracy: 0.8523 - val_loss: 0.0262 - val_accuracy: 0.8630 - 938ms/epoch - 2ms/step\n",
      "Epoch 21/50\n",
      "469/469 - 1s - loss: 0.0269 - accuracy: 0.8548 - val_loss: 0.0255 - val_accuracy: 0.8654 - 944ms/epoch - 2ms/step\n",
      "Epoch 22/50\n",
      "469/469 - 1s - loss: 0.0263 - accuracy: 0.8578 - val_loss: 0.0249 - val_accuracy: 0.8682 - 946ms/epoch - 2ms/step\n",
      "Epoch 23/50\n",
      "469/469 - 1s - loss: 0.0257 - accuracy: 0.8598 - val_loss: 0.0243 - val_accuracy: 0.8703 - 942ms/epoch - 2ms/step\n",
      "Epoch 24/50\n",
      "469/469 - 1s - loss: 0.0252 - accuracy: 0.8621 - val_loss: 0.0238 - val_accuracy: 0.8719 - 947ms/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "469/469 - 1s - loss: 0.0247 - accuracy: 0.8639 - val_loss: 0.0234 - val_accuracy: 0.8741 - 979ms/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "469/469 - 1s - loss: 0.0242 - accuracy: 0.8658 - val_loss: 0.0229 - val_accuracy: 0.8761 - 1s/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "469/469 - 1s - loss: 0.0238 - accuracy: 0.8674 - val_loss: 0.0225 - val_accuracy: 0.8772 - 1s/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "469/469 - 1s - loss: 0.0234 - accuracy: 0.8687 - val_loss: 0.0222 - val_accuracy: 0.8777 - 1s/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "469/469 - 1s - loss: 0.0231 - accuracy: 0.8698 - val_loss: 0.0218 - val_accuracy: 0.8785 - 1s/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "469/469 - 1s - loss: 0.0227 - accuracy: 0.8708 - val_loss: 0.0215 - val_accuracy: 0.8793 - 1s/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "469/469 - 1s - loss: 0.0224 - accuracy: 0.8722 - val_loss: 0.0212 - val_accuracy: 0.8810 - 1s/epoch - 2ms/step\n",
      "Epoch 32/50\n",
      "469/469 - 1s - loss: 0.0221 - accuracy: 0.8736 - val_loss: 0.0209 - val_accuracy: 0.8820 - 1s/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "469/469 - 1s - loss: 0.0218 - accuracy: 0.8745 - val_loss: 0.0207 - val_accuracy: 0.8826 - 1s/epoch - 2ms/step\n",
      "Epoch 34/50\n",
      "469/469 - 1s - loss: 0.0216 - accuracy: 0.8758 - val_loss: 0.0204 - val_accuracy: 0.8835 - 1s/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "469/469 - 1s - loss: 0.0213 - accuracy: 0.8767 - val_loss: 0.0202 - val_accuracy: 0.8850 - 1s/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "469/469 - 1s - loss: 0.0211 - accuracy: 0.8777 - val_loss: 0.0199 - val_accuracy: 0.8859 - 1s/epoch - 2ms/step\n",
      "Epoch 37/50\n",
      "469/469 - 1s - loss: 0.0209 - accuracy: 0.8786 - val_loss: 0.0197 - val_accuracy: 0.8866 - 1s/epoch - 2ms/step\n",
      "Epoch 38/50\n",
      "469/469 - 1s - loss: 0.0207 - accuracy: 0.8794 - val_loss: 0.0195 - val_accuracy: 0.8879 - 1s/epoch - 2ms/step\n",
      "Epoch 39/50\n",
      "469/469 - 1s - loss: 0.0205 - accuracy: 0.8802 - val_loss: 0.0194 - val_accuracy: 0.8888 - 1s/epoch - 2ms/step\n",
      "Epoch 40/50\n",
      "469/469 - 1s - loss: 0.0203 - accuracy: 0.8813 - val_loss: 0.0192 - val_accuracy: 0.8899 - 1s/epoch - 2ms/step\n",
      "Epoch 41/50\n",
      "469/469 - 1s - loss: 0.0201 - accuracy: 0.8820 - val_loss: 0.0190 - val_accuracy: 0.8907 - 1s/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "469/469 - 1s - loss: 0.0199 - accuracy: 0.8827 - val_loss: 0.0188 - val_accuracy: 0.8911 - 1s/epoch - 2ms/step\n",
      "Epoch 43/50\n",
      "469/469 - 1s - loss: 0.0198 - accuracy: 0.8832 - val_loss: 0.0187 - val_accuracy: 0.8920 - 1s/epoch - 2ms/step\n",
      "Epoch 44/50\n",
      "469/469 - 1s - loss: 0.0196 - accuracy: 0.8839 - val_loss: 0.0185 - val_accuracy: 0.8933 - 1s/epoch - 2ms/step\n",
      "Epoch 45/50\n",
      "469/469 - 1s - loss: 0.0195 - accuracy: 0.8844 - val_loss: 0.0184 - val_accuracy: 0.8938 - 1s/epoch - 2ms/step\n",
      "Epoch 46/50\n",
      "469/469 - 1s - loss: 0.0193 - accuracy: 0.8852 - val_loss: 0.0183 - val_accuracy: 0.8943 - 1s/epoch - 2ms/step\n",
      "Epoch 47/50\n",
      "469/469 - 1s - loss: 0.0192 - accuracy: 0.8859 - val_loss: 0.0181 - val_accuracy: 0.8948 - 1s/epoch - 2ms/step\n",
      "Epoch 48/50\n",
      "469/469 - 1s - loss: 0.0190 - accuracy: 0.8863 - val_loss: 0.0180 - val_accuracy: 0.8954 - 1s/epoch - 2ms/step\n",
      "Epoch 49/50\n",
      "469/469 - 1s - loss: 0.0189 - accuracy: 0.8871 - val_loss: 0.0179 - val_accuracy: 0.8954 - 1s/epoch - 2ms/step\n",
      "Epoch 50/50\n",
      "469/469 - 1s - loss: 0.0188 - accuracy: 0.8873 - val_loss: 0.0178 - val_accuracy: 0.8960 - 1s/epoch - 2ms/step\n",
      "정확률= 89.60000276565552\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.datasets as ds\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# [1] 데이터셋 로드\n",
    "# MNIST 손글씨 데이터셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = ds.mnist.load_data()\n",
    "\n",
    "# [2] 데이터 전처리\n",
    "# 28x28 이미지를 784 벡터로 평탄화\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "# 픽셀 값을 0~1 범위로 정규화\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# 레이블(정답)을 one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# [3] MLP 모델 구성\n",
    "mlp = Sequential()  # Sequential API 사용\n",
    "\n",
    "# 은닉층 (입력: 784, 출력: 512), 활성 함수: tanh\n",
    "mlp.add(Dense(units=512, activation='tanh', input_shape=(784,)))\n",
    "\n",
    "# 출력층 (클래스: 10개), 활성 함수: softmax\n",
    "mlp.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# [4] 모델 컴파일\n",
    "# 손실 함수: MSE (보통 다중분류는 categorical_crossentropy 사용 권장)\n",
    "# 최적화 알고리즘: SGD(확률적 경사하강법)\n",
    "# 평가 지표: 정확도\n",
    "mlp.compile(loss='MSE',\n",
    "            optimizer=SGD(learning_rate=0.01),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# [5] 학습 실행\n",
    "# 배치 사이즈: 128, 에폭: 50회, 검증 데이터: 테스트셋 사용\n",
    "mlp.fit(x_train, y_train,\n",
    "        batch_size=128,\n",
    "        epochs=50,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=2)\n",
    "\n",
    "# [6] 모델 평가\n",
    "# 테스트셋에 대한 정확도 출력\n",
    "res = mlp.evaluate(x_test, y_test, verbose=0)\n",
    "print('정확률 =', res[1] * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193d5e4",
   "metadata": {},
   "source": [
    "# 활성 함수(Activation Function) 정리\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 활성 함수란?\n",
    "\n",
    "- 뉴런의 출력값을 결정하는 함수.\n",
    "- 인공 뉴런에서 입력의 가중합을 비선형적으로 변환해줌.\n",
    "- 비선형성을 추가해야 신경망이 복잡한 패턴(비선형 문제)을 학습할 수 있음.\n",
    "- 활성 함수가 없으면 모든 층이 선형 결합만 되어버려서 아무리 깊어도 선형 모델과 같음!\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 대표적인 활성 함수 종류\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2-1. Step Function (계단 함수)\n",
    "\n",
    "- **공식**: 출력은 0 또는 1로 이산적.\n",
    "- **특징**: 최초 퍼셉트론에서 사용.\n",
    "- **한계**: 미분 불가능 → 역전파 불가능.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2-2. Sigmoid\n",
    "\n",
    "- **공식**: `f(x) = 1 / (1 + exp(-x))`\n",
    "- 출력 범위: (0, 1)\n",
    "- 확률값처럼 해석 가능.\n",
    "- **한계**:\n",
    "  - Gradient Vanishing 문제 발생 (큰 입력값에서는 기울기가 0에 가까워짐)\n",
    "  - 출력이 0 또는 1에 가까워지면 학습 속도 저하.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2-3. Tanh (Hyperbolic Tangent)\n",
    "\n",
    "- **공식**: `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
    "- 출력 범위: (-1, 1)\n",
    "- Sigmoid보다 중심이 0이라 성능이 좀 더 좋음.\n",
    "- **한계**: 여전히 Gradient Vanishing 문제 존재.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2-4. ReLU (Rectified Linear Unit)\n",
    "\n",
    "- **공식**: `f(x) = max(0, x)`\n",
    "- 음수 입력은 0, 양수는 그대로 통과.\n",
    "- 현재 가장 널리 사용됨 (CNN, MLP 모두)\n",
    "- 계산이 단순해 학습 속도가 빠름.\n",
    "- **한계**: Dead ReLU 문제 (음수 입력이 계속되면 뉴런이 죽어서 gradient가 0)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2-5. Leaky ReLU\n",
    "\n",
    "- **공식**: `f(x) = x if x > 0 else αx` (보통 α=0.01)\n",
    "- Dead ReLU 문제를 일부 완화.\n",
    "- 음수 구간에도 작은 기울기 존재.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2-6. Softmax\n",
    "\n",
    "- 다중 클래스 분류의 출력층에서 사용.\n",
    "- 각 클래스에 대한 확률값 출력.\n",
    "- 모든 출력의 합이 1이 됨.\n",
    "- **공식**:  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 활성 함수 비교 표\n",
    "\n",
    "| 함수       | 출력 범위 | 장점                         | 단점                           | 주로 사용되는 곳         |\n",
    "|------------|------------|------------------------------|--------------------------------|--------------------------|\n",
    "| Step       | {0,1}      | 구조가 간단                  | 미분 불가능                    | 퍼셉트론 초기            |\n",
    "| Sigmoid    | (0,1)      | 확률 해석 가능               | Gradient Vanishing             | 출력층(이진 분류)        |\n",
    "| Tanh       | (-1,1)     | 중심이 0이라 안정적          | Gradient Vanishing             | RNN 등                   |\n",
    "| ReLU       | [0,∞)      | 빠르고 단순, 비선형성 유지   | Dead ReLU                      | CNN, MLP 은닉층          |\n",
    "| Leaky ReLU | (-∞,∞)     | Dead ReLU 완화               | Hyperparameter(α) 필요         | CNN, MLP 은닉층          |\n",
    "| Softmax    | (0,1), 합=1 | 다중 클래스 확률 출력        | 다중 클래스 외엔 부적합        | 출력층(다중 클래스 분류) |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 한 문장 요약 \n",
    "\n",
    "> **활성 함수는 신경망에 비선형성을 부여해 복잡한 패턴을 학습할 수 있게 해주는 핵심 장치**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 실전\n",
    "\n",
    "- 은닉층 : ReLU 계열 (ReLU, Leaky ReLU)\n",
    "- 출력층 : 문제에 맞게\n",
    "- 이진 분류 → Sigmoid\n",
    "- 다중 클래스 분류 → Softmax\n",
    "- 회귀 → 보통 활성 함수 X (선형)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
